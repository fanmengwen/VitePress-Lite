#!/usr/bin/env python3
"""
å‘é‡åŒ¹é…æ”¹è¿›æ•ˆæœæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æŸ¥è¯¢æ„å›¾è¯†åˆ«å’Œç»“æœè¿‡æ»¤çš„å‡†ç¡®æ€§
"""

import json
import requests
import time
from typing import Dict, List, Any
from colorama import init, Fore, Style

# åˆå§‹åŒ–coloramaç”¨äºå½©è‰²è¾“å‡º
init()

class VectorImprovementTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    def test_single_query(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæŸ¥è¯¢ç”¨ä¾‹"""
        print(f"\n{'='*60}")
        print(f"{Fore.CYAN}æµ‹è¯•: {test_case['name']}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}æŸ¥è¯¢: {test_case['query']}{Style.RESET_ALL}")
        print(f"é¢„æœŸæ„å›¾: {test_case['expected_intent']}")
        
        # å‘é€APIè¯·æ±‚
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "question": test_case['query'],
                    "include_sources": True
                },
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return self._analyze_response(test_case, data)
            else:
                return {
                    "name": test_case['name'],
                    "status": "ERROR",
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "score": 0
                }
                
        except requests.exceptions.RequestException as e:
            return {
                "name": test_case['name'],
                "status": "ERROR", 
                "error": str(e),
                "score": 0
            }
    
    def _analyze_response(self, test_case: Dict[str, Any], response_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå“åº”ç»“æœ"""
        sources = response_data.get('sources', [])
        
        # æ£€æŸ¥åº”è¯¥åŒ…å«çš„æ–‡æ¡£
        should_include = test_case.get('should_include', [])
        should_exclude = test_case.get('should_exclude', [])
        
        included_files = [source['file_path'] for source in sources]
        
        # è®¡ç®—å‡†ç¡®æ€§å¾—åˆ†
        include_score = 0
        exclude_score = 0
        
        # æ£€æŸ¥åº”è¯¥åŒ…å«çš„æ–‡æ¡£
        for should_file in should_include:
            if any(should_file in file_path for file_path in included_files):
                include_score += 1
                print(f"{Fore.GREEN}âœ“ æ­£ç¡®åŒ…å«: {should_file}{Style.RESET_ALL}")
            else:
                print(f"{Fore.RED}âœ— ç¼ºå¤±æ–‡æ¡£: {should_file}{Style.RESET_ALL}")
        
        # æ£€æŸ¥åº”è¯¥æ’é™¤çš„æ–‡æ¡£
        excluded_files = []
        for exclude_file in should_exclude:
            if any(exclude_file in file_path for file_path in included_files):
                excluded_files.append(exclude_file)
                print(f"{Fore.RED}âœ— é”™è¯¯åŒ…å«: {exclude_file}{Style.RESET_ALL}")
            else:
                exclude_score += 1
                print(f"{Fore.GREEN}âœ“ æ­£ç¡®æ’é™¤: {exclude_file}{Style.RESET_ALL}")
        
        # è®¡ç®—æ€»åˆ†
        total_checks = len(should_include) + len(should_exclude)
        correct_checks = include_score + exclude_score
        accuracy = correct_checks / total_checks if total_checks > 0 else 0
        
        # æ˜¾ç¤ºè¿”å›çš„æºæ–‡æ¡£
        print(f"\n{Fore.MAGENTA}è¿”å›çš„æºæ–‡æ¡£:{Style.RESET_ALL}")
        for i, source in enumerate(sources[:3], 1):
            score = source.get('similarity_score', 0)
            print(f"  {i}. {source['file_path']} (ç›¸ä¼¼åº¦: {score:.3f})")
        
        result = {
            "name": test_case['name'],
            "query": test_case['query'],
            "status": "SUCCESS",
            "accuracy": accuracy,
            "score": int(accuracy * 100),
            "included_correct": include_score,
            "excluded_correct": exclude_score, 
            "total_sources": len(sources),
            "confidence_score": response_data.get('confidence_score', 0),
            "response_time_ms": response_data.get('response_time_ms', 0),
            "errors": excluded_files
        }
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
        status_color = Fore.GREEN if accuracy >= 0.8 else Fore.YELLOW if accuracy >= 0.6 else Fore.RED
        print(f"\n{status_color}å‡†ç¡®ç‡: {accuracy:.1%} ({correct_checks}/{total_checks}){Style.RESET_ALL}")
        print(f"ç½®ä¿¡åº¦: {result['confidence_score']:.2f}")
        print(f"å“åº”æ—¶é—´: {result['response_time_ms']}ms")
        
        return result
    
    def run_all_tests(self, test_file: str = "test_queries.json") -> None:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹"""
        print(f"{Fore.CYAN}{'='*60}")
        print(f"  å‘é‡åŒ¹é…æ”¹è¿›æ•ˆæœæµ‹è¯•")
        print(f"{'='*60}{Style.RESET_ALL}")
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
        except FileNotFoundError:
            print(f"{Fore.RED}é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶ {test_file}{Style.RESET_ALL}")
            return
        except json.JSONDecodeError:
            print(f"{Fore.RED}é”™è¯¯: æµ‹è¯•æ–‡ä»¶æ ¼å¼æ— æ•ˆ{Style.RESET_ALL}")
            return
        
        # æ‰§è¡Œæµ‹è¯•
        for test_case in test_cases:
            result = self.test_single_query(test_case)
            self.results.append(result)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_report()
    
    def _generate_report(self) -> None:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{Fore.CYAN}{'='*60}")
        print(f"  æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print(f"{'='*60}{Style.RESET_ALL}")
        
        successful_tests = [r for r in self.results if r['status'] == 'SUCCESS']
        failed_tests = [r for r in self.results if r['status'] == 'ERROR']
        
        if successful_tests:
            avg_accuracy = sum(r['accuracy'] for r in successful_tests) / len(successful_tests)
            avg_confidence = sum(r['confidence_score'] for r in successful_tests) / len(successful_tests)
            avg_response_time = sum(r['response_time_ms'] for r in successful_tests) / len(successful_tests)
            
            print(f"âœ“ æˆåŠŸæµ‹è¯•: {len(successful_tests)}/{len(self.results)}")
            print(f"âœ“ å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.1%}")
            print(f"âœ“ å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
            print(f"âœ“ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.0f}ms")
            
            # è¯¦ç»†ç»“æœ
            print(f"\n{Fore.YELLOW}è¯¦ç»†ç»“æœ:{Style.RESET_ALL}")
            for result in successful_tests:
                status_icon = "ğŸŸ¢" if result['accuracy'] >= 0.8 else "ğŸŸ¡" if result['accuracy'] >= 0.6 else "ğŸ”´"
                print(f"  {status_icon} {result['name']}: {result['accuracy']:.1%}")
                if result['errors']:
                    print(f"    âš ï¸  é”™è¯¯åŒ…å«: {', '.join(result['errors'])}")
        
        if failed_tests:
            print(f"\n{Fore.RED}å¤±è´¥æµ‹è¯•: {len(failed_tests)}{Style.RESET_ALL}")
            for result in failed_tests:
                print(f"  âŒ {result['name']}: {result['error']}")
        
        # æ”¹è¿›å»ºè®®
        if successful_tests:
            low_accuracy_tests = [r for r in successful_tests if r['accuracy'] < 0.8]
            if low_accuracy_tests:
                print(f"\n{Fore.YELLOW}æ”¹è¿›å»ºè®®:{Style.RESET_ALL}")
                for result in low_accuracy_tests:
                    print(f"  ğŸ“ {result['name']} å‡†ç¡®ç‡è¾ƒä½ ({result['accuracy']:.1%})")
                    print(f"     å»ºè®®æ£€æŸ¥æŸ¥è¯¢æ„å›¾è¯†åˆ«å’Œè¯„åˆ†æƒé‡")

def main():
    """ä¸»å‡½æ•°"""
    print("å‘é‡åŒ¹é…æ”¹è¿›æµ‹è¯•å·¥å…·")
    print("ç¡®ä¿AIæœåŠ¡æ­£åœ¨è¿è¡Œåœ¨ http://localhost:8000")
    
    tester = VectorImprovementTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main()
